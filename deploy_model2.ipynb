{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\GitHub\\Retinal_Diseases_Classifier\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import engine\n",
    "import helper_functions\n",
    "import utils\n",
    "import data_setup\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = \"data/Train\"\n",
    "val_dir = \"data/Val\"\n",
    "test_dir = \"data/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = engine.create_pretrained_effnet_model(version=\"b3\", num_classes=5,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARMD', 'DR', 'MH', 'NORMAL', 'ODC']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transforms=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)\n",
    "class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 50\n",
      "Dataset size: 1591\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total batches: {len(train_dataloader_effnetb2)}\")\n",
    "print(f\"Dataset size: {len(train_dataloader_effnetb2.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train EffnetB2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1/10 [00:32<04:54, 32.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss : 1.0879 | train_acc : 0.5851 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.8431 | test_acc : 0.6875 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 2/10 [01:06<04:28, 33.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss : 0.6958 | train_acc : 0.7417 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.7260 | test_acc : 0.7396 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 3/10 [01:41<03:59, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss : 0.5727 | train_acc : 0.7846 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.6957 | test_acc : 0.7479 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [02:15<03:23, 33.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss : 0.4871 | train_acc : 0.8219 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.6720 | test_acc : 0.7562 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [02:49<02:49, 33.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss : 0.4215 | train_acc : 0.8274 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.6468 | test_acc : 0.7854 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [03:22<02:15, 33.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss : 0.3714 | train_acc : 0.8458 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.7189 | test_acc : 0.7667 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [03:56<01:41, 33.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss : 0.3285 | train_acc : 0.8654 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.7678 | test_acc : 0.7708 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [04:30<01:07, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss : 0.2906 | train_acc : 0.8734 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.6679 | test_acc : 0.7729 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [05:05<00:34, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss : 0.2477 | train_acc : 0.8791 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.7393 | test_acc : 0.7854 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [05:38<00:00, 33.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss : 0.2435 | train_acc : 0.8893 | train_prec : 0.0000 | train_rec : 0.0000 | train_f1 : 0.0000 | test_loss : 0.7563 | test_acc : 0.7958 | test_prec : 0.0000 | test_rec : 0.0000 | test_f1 : 0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(params=effnetb2.parameters(),lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "helper_functions.set_seeds()\n",
    "effnetb2.to(device)\n",
    "# Set seeds for reproducibility and train the model\n",
    "helper_functions.set_seeds()\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=10,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device,\n",
    "                                binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: models\\pretrained_effnetb3_type_classification.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "utils.save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"pretrained_effnetb3_type_classification.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check size of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained EffNetB3 feature extractor model size: 41 MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb2_model_size = Path(\"models/pretrained_effnetb3_type_classification.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB3 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.7562736888726552,\n",
       " 'test_acc': 0.7958333333333333,\n",
       " 'number_of_parameters': 10703917,\n",
       " 'model_size (MB)': 41}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of parameters in EffNetB2\n",
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "\n",
    "# Create a dictionary with EffNetB2 statistics\n",
    "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": effnetb2_total_params,\n",
    "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finding all filepaths ending with '.jpg' in directory: data/Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/Test/ARMD/DatasetA_110.png'),\n",
       " WindowsPath('data/Test/ARMD/DatasetA_12.png'),\n",
       " WindowsPath('data/Test/ARMD/DatasetA_122.png'),\n",
       " WindowsPath('data/Test/ARMD/DatasetA_152.png'),\n",
       " WindowsPath('data/Test/ARMD/DatasetA_188.png')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.png\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function to make predictions and store results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions with effnetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480/480 [01:03<00:00,  7.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions across test dataset with EffNetB2\n",
    "import predict\n",
    "effnetb2_test_pred_dicts = predict.pred_and_store(paths=test_data_paths,\n",
    "                                          model=effnetb2,\n",
    "                                          transform=effnetb2_transforms,\n",
    "                                          class_names=class_names,\n",
    "                                          device=\"cpu\") # make predictions on CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_path': WindowsPath('data/Test/ARMD/DatasetA_110.png'),\n",
       "  'class_name': 'ARMD',\n",
       "  'pred_prob': 0.8837,\n",
       "  'pred_class': 'ARMD',\n",
       "  'time_for_pred': 0.1752,\n",
       "  'correct': True},\n",
       " {'image_path': WindowsPath('data/Test/ARMD/DatasetA_12.png'),\n",
       "  'class_name': 'ARMD',\n",
       "  'pred_prob': 0.7983,\n",
       "  'pred_class': 'ARMD',\n",
       "  'time_for_pred': 0.1136,\n",
       "  'correct': True}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pandas Dataframe to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class_name</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>time_for_pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\Test\\ARMD\\DatasetA_110.png</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\Test\\ARMD\\DatasetA_12.png</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.7983</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\Test\\ARMD\\DatasetA_122.png</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\Test\\ARMD\\DatasetA_152.png</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\Test\\ARMD\\DatasetA_188.png</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.7714</td>\n",
       "      <td>ARMD</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        image_path class_name  pred_prob pred_class  \\\n",
       "0  data\\Test\\ARMD\\DatasetA_110.png       ARMD     0.8837       ARMD   \n",
       "1   data\\Test\\ARMD\\DatasetA_12.png       ARMD     0.7983       ARMD   \n",
       "2  data\\Test\\ARMD\\DatasetA_122.png       ARMD     0.5399       ARMD   \n",
       "3  data\\Test\\ARMD\\DatasetA_152.png       ARMD     0.9496       ARMD   \n",
       "4  data\\Test\\ARMD\\DatasetA_188.png       ARMD     0.7714       ARMD   \n",
       "\n",
       "   time_for_pred  correct  \n",
       "0         0.1752     True  \n",
       "1         0.1136     True  \n",
       "2         0.0923     True  \n",
       "3         0.0935     True  \n",
       "4         0.0990     True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct\n",
       "True     382\n",
       "False     98\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffNetB2 average time per prediction: 0.1309 seconds\n"
     ]
    }
   ],
   "source": [
    "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gradio Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs -> ML model -> outputs  \n",
    "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put EffNetB2 on CPU\n",
    "effnetb2.to(\"cpu\") \n",
    "\n",
    "# Check the device\n",
    "next(iter(effnetb2.parameters())).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    start_time = timer()\n",
    "    \n",
    "    # transform image and add batch dim\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # put model in eval mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim = 1)\n",
    "        \n",
    "    # create prediction label and prediction prob\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    return pred_labels_and_probs, pred_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Predicting on image at path: data\\Test\\DISEASE\\DR\\DatasetA_480.png\n",
      "\n",
      "Prediction label and probability dictionary: \n",
      "{'ARMD': 0.10593947768211365, 'DR': 0.8825823664665222, 'MH': 0.000907509122043848, 'ODC': 0.010570685379207134}\n",
      "Prediction time: 0.47223 seconds\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Randomly select a test image path\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "# open image\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
    "\n",
    "# Predict on the target image and print out the outputs\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
    "print(f\"Prediction time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['data\\\\Test\\\\DISEASE\\\\ARMD\\\\DatasetA_38.png'],\n",
       " ['data\\\\Test\\\\DISEASE\\\\DR\\\\DatasetA_415.png'],\n",
       " ['data\\\\Test\\\\DISEASE\\\\DR\\\\DatasetA_169.png']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of example inputs to our Gradio demo\n",
    "import random\n",
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create title, description and article strings\n",
    "title = \"FoodVision Mini üçïü•©üç£\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndemo = gr.Interface(fn=predict, # mapping function from input to output\\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\\n                    examples=example_list, \\n                    title=title,\\n                    description=description,\\n                    article=article)\\n                    '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Gradio demo\n",
    "\"\"\"\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the demo!\n",
    "#demo.launch(debug=False, # print errors locally?\n",
    "#           share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Demo Directory to deploy on Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create FoodVision mini demo path\n",
    "foodvision_mini_demo_path = Path(\"demos/retinal_disease_classification/\")\n",
    "\n",
    "# Remove files that might already exist there and create new directory\n",
    "if foodvision_mini_demo_path.exists():\n",
    "    shutil.rmtree(foodvision_mini_demo_path)\n",
    "# If the file doesn't exist, create it anyway\n",
    "foodvision_mini_demo_path.mkdir(parents=True, \n",
    "                                exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Copying data\\pizza_steak_sushi_20_percent\\test\\sushi\\592799.jpg to demos\\foodvision_mini\\examples\\592799.jpg\n",
      "[INFO] Copying data\\pizza_steak_sushi_20_percent\\test\\steak\\3622237.jpg to demos\\foodvision_mini\\examples\\3622237.jpg\n",
      "[INFO] Copying data\\pizza_steak_sushi_20_percent\\test\\pizza\\2582289.jpg to demos\\foodvision_mini\\examples\\2582289.jpg\n"
     ]
    }
   ],
   "source": [
    "# 1. Create an examples directory\n",
    "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Collect three random test dataset image paths\n",
    "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
    "\n",
    "# 3. Copy the three random images to the examples directory\n",
    "for example in foodvision_mini_examples:\n",
    "    destination = foodvision_mini_examples_path / example.name\n",
    "    print(f\"[INFO] Copying {example} to {destination}\")\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['examples/2582289.jpg'], ['examples/3622237.jpg'], ['examples/592799.jpg']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get example filepaths in a list of lists\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos\\foodvision_mini\\09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
      "[INFO] No model found at models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth, perhaps its already been moved?\n",
      "[INFO] Model exists at demos\\foodvision_mini\\09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth: True\n"
     ]
    }
   ],
   "source": [
    "# Create a source path for our target model\n",
    "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
    "\n",
    "# Create a destination path for our target model \n",
    "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "# Try to move the file\n",
    "try:\n",
    "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
    "    \n",
    "    # Move the model\n",
    "    shutil.move(src=effnetb2_foodvision_mini_model_path, \n",
    "                dst=effnetb2_foodvision_mini_model_destination)\n",
    "    \n",
    "    print(f\"[INFO] Model move complete.\")\n",
    "\n",
    "# If the model has already been moved, check if it exists\n",
    "except:\n",
    "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
    "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Compress-Archive' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
